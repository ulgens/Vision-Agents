# HeyGen Avatar Examples

This directory contains examples of how to use the HeyGen plugin to add realistic avatar video to your AI agent.

## Examples

### 1. Standard Streaming LLM (`avatar_example.py`)

Uses a standard streaming LLM (Gemini) with separate TTS/STT components. Best for traditional text-based LLMs.

### 2. Realtime LLM (`avatar_realtime_example.py`)

Uses Gemini Realtime with native audio input/output. The avatar lip-syncs to the transcribed text while Gemini handles voice processing.

## Setup

1. **Install dependencies:**

```bash
cd plugins/heygen/example
uv pip install -e .
```

2. **Configure environment variables:**

Copy `.env.example` to `.env` and fill in your API keys:

```bash
cp .env.example .env
```

**For Standard Example** (`avatar_example.py`):
- `HEYGEN_API_KEY` - Get from [HeyGen](https://heygen.com)
- `STREAM_API_KEY` and `STREAM_SECRET` - Get from [GetStream](https://getstream.io)
- `CARTESIA_API_KEY` - Get from [Cartesia](https://cartesia.ai)
- `DEEPGRAM_API_KEY` - Get from [Deepgram](https://deepgram.com)
- `GOOGLE_API_KEY` - Get from [Google AI Studio](https://makersuite.google.com/app/apikey)

**For Realtime Example** (`avatar_realtime_example.py`):
- `HEYGEN_API_KEY` - Get from [HeyGen](https://heygen.com)
- `STREAM_API_KEY` and `STREAM_SECRET` - Get from [GetStream](https://getstream.io)
- `GOOGLE_API_KEY` - Get from [Google AI Studio](https://makersuite.google.com/app/apikey)

## Running the Examples

From the project root:

**Standard Streaming LLM:**
```bash
uv run plugins/heygen/example/avatar_example.py
```

**Realtime LLM:**
```bash
uv run plugins/heygen/example/avatar_realtime_example.py
```

Both will:
1. Start an AI agent with a HeyGen avatar
2. Open a demo UI in your browser
3. The avatar will speak and be ready to chat

## How It Works

### Standard Streaming LLM (`avatar_example.py`)

1. **Agent Setup**: The agent is configured with:
   - Gemini LLM for generating responses
   - Cartesia TTS for speech synthesis
   - Deepgram STT for speech recognition
   - HeyGen AvatarPublisher for avatar video

2. **Avatar Streaming**: When the agent speaks:
   - Text is generated by Gemini LLM
   - Text is sent to HeyGen for lip-sync
   - Audio is synthesized by Cartesia TTS
   - HeyGen generates avatar video with lip-sync
   - Avatar video and audio are streamed to the call

3. **User Interaction**: When you speak:
   - Audio is captured from your microphone
   - Transcribed to text by Deepgram
   - Sent to Gemini LLM for processing
   - Response is generated and spoken through the avatar

### Realtime LLM (`avatar_realtime_example.py`)

1. **Agent Setup**: The agent is configured with:
   - Gemini Realtime for native audio processing
   - HeyGen AvatarPublisher for avatar video

2. **Avatar Streaming**: When the agent speaks:
   - Gemini Realtime generates audio directly (24kHz PCM)
   - Text transcription is sent to HeyGen for lip-sync
   - HeyGen generates avatar video with lip-sync
   - Gemini's audio is used (HeyGen audio is not forwarded for Realtime LLMs)
   - Avatar video and Gemini audio are streamed to the call

3. **User Interaction**: When you speak:
   - Audio is captured and sent directly to Gemini Realtime
   - Gemini processes audio natively (no separate STT needed)
   - Response is generated and spoken through the avatar

## Customization

### Using a Different Avatar

Get your avatar ID from HeyGen dashboard and update:

```python
from vision_agents.plugins.heygen import VideoQuality

heygen.AvatarPublisher(
    avatar_id="your_avatar_id_here",
    quality=VideoQuality.HIGH
)
```

### Adjusting Video Quality

Choose quality based on your bandwidth:

```python
from vision_agents.plugins.heygen import VideoQuality

heygen.AvatarPublisher(
    avatar_id="default",
    quality=VideoQuality.LOW,  # Options: VideoQuality.LOW, VideoQuality.MEDIUM, or VideoQuality.HIGH
    resolution=(1280, 720)  # Lower resolution for better performance
)
```

### Using a Different LLM

**With Standard Streaming LLM:**
```python
from vision_agents.plugins import openai, elevenlabs

agent = Agent(
    edge=getstream.Edge(),
    agent_user=User(name="Avatar AI"),
    instructions="Your instructions here",
    llm=openai.LLM("gpt-4"),
    tts=elevenlabs.TTS(),
    stt=deepgram.STT(),
    processors=[
        heygen.AvatarPublisher(avatar_id="default")
    ]
)
```

**With Realtime LLM:**
```python
from vision_agents.plugins import openai

agent = Agent(
    edge=getstream.Edge(),
    agent_user=User(name="Avatar AI"),
    instructions="Your instructions here",
    llm=openai.Realtime(model="gpt-4o-realtime-preview"),
    processors=[
        heygen.AvatarPublisher(
            avatar_id="default"
        )
    ]
)
```

## Troubleshooting

### "HeyGen API key required" Error

Make sure `HEYGEN_API_KEY` is set in your `.env` file.

### Connection Timeout

- Check your internet connection
- Verify HeyGen API key is valid
- Ensure firewall allows WebRTC traffic

### No Video Appearing

- Check browser console for errors
- Verify GetStream credentials are correct
- Try lowering video quality settings

## Learn More

- [HeyGen API Documentation](https://docs.heygen.com/docs/streaming-api)
- [Vision Agents Documentation](https://visionagents.ai/)
- [GetStream Video Documentation](https://getstream.io/video/docs/)
